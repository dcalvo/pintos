+--------------------+
			|        CS 318      |
			| PROJECT 1: THREADS |
			|   DESIGN DOCUMENT  |
			+--------------------+
				   
---- GROUP ----

>> Fill in the names and email addresses of your group members.

David Calvo <dcalvo2@domain.example>
Vivian Sanchez <vsanche4@domain.example>
Ken Tana <ptanawa1@domain.example>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

Online sources:
https://www.thegeekstuff.com/2013/04/c-macros-inline-functions/
https://www.geeksforgeeks.org/write-header-file-c/
https://www.eskimo.com/~scs/cclass/int/sx6a.html
https://stackoverflow.com/questions/54884274/can-i-define-a-macro-in-a-header-file

			     ALARM CLOCK
			     ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

static struct condition alarm;
Sleeping threads wait on “alarm”. When a thread signals “alarm”, a sleeping thread wakes up.

static struct list alarm_list;
alarm_list is a sorted list of alarms. If the timer passes the first alarm, then all sleeping threads will wake up.

static struct lock lock;
A thread holds this lock to use “alarm”.

struct value
  {
    int64_t value;
    struct list_elem elem;
  };
struct value is a list element that contains an int64_t. (alarm_list is a list of struct values.)

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

In timer_sleep, a thread will
    (1) acquire a lock,
    (2) create an alarm, and
    (3) call cond_wait (releasing the lock).
When another thread calls cond_signal (after at least TICKS timer ticks), the thread will
    (4) exit cond_wait (acquiring the lock), and
    (5) release the lock.
The lock prevents threads from adding alarms to alarm_list at the same time, thereby protecting the alarm_list from timer_interrupt.

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

We didn’t add any code to timer_interrupt. timer_wake() is called by thread_block(), thread_exit(), and thread_yield().

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

A lock prevents threads from adding alarms to alarm_list at the same time, thereby avoiding race conditions.

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

The lock prevents threads from adding alarms to alarm_list at the same time, thereby protecting the alarm_list from timer_interrupt.

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

Initially, we wanted timer_wake() to be called by timer_interrupt, but timer_wake() couldn’t lock_acquire in an interrupt context. Then, we tried calling timer_wake() in schedule, but the current thread’s status has to be THREAD_RUNNING. So, we added calls to timer_wake() in thread_block(), thread_exit(), and thread_yield().

			 PRIORITY SCHEDULING
			 ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

/* Lock. */
struct lock 
  {
    struct thread *holder;      /* Thread holding lock (for debugging). */
    struct semaphore semaphore; /* Binary semaphore controlling access. */
    struct list_elem elem;      /* List element for lock holder. */
    int priority;               /* Priority equivalent to the highest waiter priority. */
  };
Added priority for priority donation.

/* One semaphore in a list. */
struct semaphore_elem 
  {
    struct list_elem elem;              /* List element. */
    struct semaphore semaphore;         /* This semaphore. */
    int priority;                       /* Priority. */
  };
Added priority for priority scheduling of semaphores.

struct thread
  {
    /* Owned by thread.c. */
    tid_t tid;                          /* Thread identifier. */
    enum thread_status status;          /* Thread state. */
    char name[16];                      /* Name (for debugging purposes). */
    uint8_t *stack;                     /* Saved stack pointer. */
    int priority;                       /* Priority. */
    int base_priority;                  /* Underlying priority, without donations. */
    struct list_elem allelem;           /* List element for all threads list. */

    /* Shared between thread.c and synch.c. */
    struct list_elem elem;              /* List element. */
    struct list locks;                  /* A list of locks the thread has acquired. */
    struct lock *requesting;             /* The Lock that a thread is requesting to acquire. */

#ifdef USERPROG
    /* Owned by userprog/process.c. */
    uint32_t *pagedir;                  /* Page directory. */
#endif
    /* Owned by thread.c. */
    unsigned magic;                     /* Detects stack overflow. */
  };
Added a list of locks and the lock that the thread is currently requesting for priority donation.

>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)
Instead of using recursion, I used the lock as the primary holder of the donated priority, which it could then donate to its holder. In an example, if thread L acquires a lock and then thread H attempts to acquire it, it will increase the priority of the lock to H’s priority, which will in turn raise L’s priority to H’s priority. The way that I do nested priority donations is by following the requesting chain. By continually following the thread->requesting->holder->requesting… chain I can update the priorities as far as necessary without the overhead of a recursive call. It seemed neater.

Please see the attached nested.png for a diagram of how this works.

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?
Primarily, this is done through my implementation of a priority queue. Depending on the type of resource, I have different comparators to obtain the priority of that resource. I ensure that pushing and popping elements is always done in an ordered fashion, while also ensuring to re-sort the linked list if I update an internal element's priority. The best examples of these functions are priority_greater_comp (which is used to compare thread priority), cond_greater_comp (for semaphores attached to a condition), and lock_greater_comp (which is for locks, naturally). During sema_up, we just pop from the correct list and given the consistency of my functions and implementation, we’ll always get the highest priority thread (including with updated donated priorities).

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?
Here’s some pseudocode first:
lock_acquire
	thread officially requests the lock
	thread attempts to acquire (sema_down)
		if lock is currently held
			add thread to the lock's waiters
			while there is a lock being requested
				update the lock to its highest waiter priority
				update the lock holder's priority to its highest lock priority
				set the current lock to the lock holder's requested lock
			end while
		end if
	add the acquired lock to this thread's held locks
	update the lock's holder to be this thread
	clear our request for the lock
end lock_acquire

This is almost line for line what is happening in my lock_acquire and sema_down functions. The interesting portion is the while loop which is used to handle nested donations. Instead of recursively calling some donate function, we simply update each lock’s priority in the chain. This is accomplished through the “requesting” member of the thread struct. When a thread officially requests a lock, it will point the requesting member to it. We then use this in sema_down to follow the chain of resource requests. By dereferencing requesting, we get a lock, which has a holder, which is also requesting a lock, which has a holder, etc. 

“ lock = lock->holder->requesting; “ is the magic line.

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.
lock_release
	thread removes lock from its held list
	thread updates its priority to new list of held locks
	set lock holder to NULL
	update the lock to its highest waiter priority
	sema_up
		re-sort the list of lock waiters
		unblock highest priority waiter

As you can trace through the pseudocode, if low priority thread L releases the lock, it will have had its priority updated by high priority thread H, so it must update it again. The order of operations is important to ensure we have the pointers to the resources we want to update. First, we remove the lock from its held list then update the thread’s priority (which could only keep it the same or lower it, since we were using the highest priority in the list). Then, we set the lock’s holder to null and update the lock’s priority to its highest waiter’s priority (which should still be H’s priority). Then, we up the semaphore and let the next thread use it. However, before doing that, we must critically update the lock’s waiter list and ensure that the highest priority thread is at the front (priorities may have changed due to releasing the lock). 

The high priority thread should then acquire the lock through sema_down and clear its request for the lock, severing the donation chain (which will update when the priority releases the resources, just in time for the next thread to acquire it with the appropriate priority). 

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?
There would be a potential race condition if a thread with a lower priority sets its own priority to be higher than that of the next thread (which was scheduled to run). Some strange behavior could arise if that thread was holding a lock and it would essentially create a priority inversion with no solution other than to set the threads’ priorities again (which I do not think could be done in that situation). The way I combat this is through two simple steps. First, after a thread sets its priority (which is really only affecting its base priority), I update its priority based off of the locks it is holding. I increase the priority if necessary, but never lower it below the thread’s base priority. Second, I immediately call my priority_check function which ensures that if the next thread is higher priority, it will run immediately.

The benefit to this is that if high priority thread H1 acquires a lock, then high priority thread H2 blocks on acquiring the lock, when H1 lowers its priority (which would normally gridlock the system), it will still maintain the updated lock holder priority due to H2 requesting the resource. Thus, when priority_check is called, H2 is not scheduled next, but H1 will be. Therefore, it can release the lock when it is done with the resource, drop its priority to what it set it to, and H2 can acquire the lock as intended.

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?
I think the largest deviation in my design from the suggested implementation is the priority donation system. The project documentation seemed to suggest that I use a recursive approach with a depth limit, but that seemed rather clunky and dangerous regarding memory for such a small OS. Recursive calls have a lot of memory overhead due to the stack frames, while in contrast, my method of following requesting->holder trails is very efficient in both memory and operations.

The entire implementation only required two lists: one for the lock’s waiters (which we already have through the embedded semaphore), and one for the thread’s held locks. By associating a priority with the lock, which is derived from the lock’s highest priority waiter, we can completely detach the donation of priority from the initial process. Any thread who holds that lock will have that priority, if it is beneficial to them, as long as that waiter is requesting it. The effect is exactly the same as a recursive method, but much more elegant conceptually and programmatically. It just seemed neater.

			  ADVANCED SCHEDULER
			  ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

Global variable
	int load_avg;    This is needed to keep track of the system load average.

Changed struct members are
struct thread {
	int nice;	         This is needed to store the nice value for each thread.
    int recent_cpu;      This is needed to store the recent_cpu calculation for each thread.
}

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu     priority       thread
ticks   A   B   C         A   B  C      to run
-----    --   --   --    --   --  --   ------        keeping track of queue
 0       0   0    0      63  61 59       A            [B, C]
 4       4   0    0      62  61 59       A            [B, C]
 8       8   0    0      61  61 59       B            [A, C]
12       8    4    0     61 60  59       A            [B, C]
16       12   4    0     60 60  59       B            [A, C]
20       12   8    0     60 59  59       A            [C, B]
24       16   8    0     59 59  59       C            [B, A]
28       16   8    4     59 59  58       B            [A, C]
32       16  12    4     59 58  58       A            [C, B]  
36       20  12    4     58 58  58       C            [B, A]

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

Yes, this is uncertain when priorities are the same value. The rule for MLFQS is when two threads have the same priority, the first thread in the ready list should run.

My advanced scheduler is not working completely so this behavior doesn’t exactly match.

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

Since the scheduling code is done mostly inside the interrupt context, performance will be slow.

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

This design is essentially a direct translation of the formulas and sentence guides from the 4BSD scheduler website provided, so it is as simple as it can possibly be, but the design doesn’t work fully as shown by the test cases.

I think a disadvantage of this design is that most of the scheduling happens in the interrupt context, but I’m not entirely sure how you could take it out of the interrupt context. Another disadvantage of this design could be that the calculations could probably be more efficient.

Since my design is not fully working, I would need to figure out how to fix it first. I’m not sure if my fixed pointer arithmetic functions are buggy or if there are other issues. If I had more time, I would figure out what is wrong with the design.

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

I read the design document and the FAQ before implementing the arithmetic routines for fixed-point math. Macros were suggested as an option in the C6 rationale question to implement these routines (and a fixed-point.h file was noted in the FAQs reference solution), so I chose to use macros when creating the arithmetic functions and put them in fixed-point.h, because in my opinion macros & function-like macros are more readable than a set of functions since their definitions use less line space.
When implementing macros in fixed-point.h, I was originally thinking about their benefits in terms of readability, but while I was looking at some sources listed above, it was mentioned that function-like macros also avoid function call overhead. To conclude, I didn’t use a set of functions because of stylistic and efficiency reasons, and I used regular and function-like macros because they seemed to be more readable and avoid function call overhead.

			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?
